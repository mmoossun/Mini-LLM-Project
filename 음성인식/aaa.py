import streamlit as st
import whisper
import speech_recognition as sr
from langchain_openai import AzureChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv
import os

# Whisper ëª¨ë¸ ë¡œë“œ
model = whisper.load_model("base")
file_name = "recorded_audio.wav"

# Recognizer ê°ì²´ ìƒì„±
r = sr.Recognizer()

def transcribe_audio(audio_file):
    # Whisper ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    audio_bytes = audio_file.read()
    with open(file_name, "wb") as f:
        f.write(audio_bytes)
    result = model.transcribe(file_name)
    return result['text']

def main():
    st.title("ìŒì„± íŒŒì¼ ê¸°ë°˜ ì±—ë´‡ ë° íšŒì˜ë¡ ì‘ì„±ê¸°")

    # ìŒì„± íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
    def get_audio_input(audio_file_path):
        with open(audio_file_path, "rb") as audio_file:
            text = transcribe_audio(audio_file)
        if text:
            return text
        else:
            return None

    # ì±—ë´‡ ì‘ë‹µì„ ì–»ëŠ” í•¨ìˆ˜
    def get_chatbot_response(user_input):
        return f"'{user_input}'"

    def record_audio():
        with sr.Microphone() as source:
            st.write("ë…¹ìŒ ì‹œì‘...")
            audio_data = r.listen(source)
            st.write("ë…¹ìŒ ì™„ë£Œ!")
            return audio_data

    # ìƒíƒœ ì´ˆê¸°í™”
    if "recorded" not in st.session_state:
        st.session_state.recorded = False
    if "all_transcripts" not in st.session_state:
        st.session_state.all_transcripts = ""
    if "summaries" not in st.session_state:
        st.session_state.summaries = []

    if st.button("ìŒì„± ë…¹ìŒ ì‹œì‘") and not st.session_state.recorded:
        audio_data = record_audio()

        # ì˜¤ë””ì˜¤ íŒŒì¼ë¡œ ì €ì¥
        with open(file_name, "wb") as f:
            f.write(audio_data.get_wav_data())
        
        st.success(f"ë…¹ìŒëœ íŒŒì¼ì´ {file_name}ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        st.session_state.recorded = True

    if st.session_state.recorded:
        # ë…¹ìŒëœ íŒŒì¼ì˜ ê²½ë¡œë¥¼ uploaded_file ë³€ìˆ˜ì— í• ë‹¹
        uploaded_file = file_name

        if uploaded_file is not None:
            user_input = get_audio_input(uploaded_file)
            if user_input is not None:
                st.session_state.all_transcripts += user_input + "\n"
                st.text_area("ëª¨ë“  ë…¹ìŒ ë‚´ìš©:", st.session_state.all_transcripts, height=200)
                
                st.session_state.recorded = False

    if st.button("ìš”ì•½í•˜ê¸°") and st.session_state.all_transcripts:
        load_dotenv()

        meeting_notes = st.session_state.all_transcripts
        prompt = ChatPromptTemplate.from_template("summarize this meeting minutes '{meeting_notes}' in korean")

        model = AzureChatOpenAI(
            azure_deployment=os.getenv("AZURE_OPENAI_DEPLOYMENT"),
            openai_api_version=os.getenv("OPENAI_API_VERSION")
        )
        output_parser = StrOutputParser()

        chain = prompt | model | output_parser

        out = chain.invoke({"meeting_notes": meeting_notes})

        # íšŒì˜ë¡ ìš”ì•½ì„ ì„¸ì…˜ ìƒíƒœì— ì¶”ê°€
        st.session_state.summaries.append(out)

        # ëª¨ë“  ìš”ì•½ë³¸ì„ í™”ë©´ì— í‘œì‹œ
        st.header("íšŒì˜ë¡ ìš”ì•½")
        for idx, summary in enumerate(st.session_state.summaries):
            st.text_area(f"íšŒì˜ë¡ ìš”ì•½ {idx + 1}:", summary, height=200)

        # ìµœì¢… ìš”ì•½ë³¸ ìƒì„±
        final_summary = "\n".join(st.session_state.summaries)

        # ìµœì¢… ìš”ì•½ë³¸ì„ í™”ë©´ì— í‘œì‹œ
        st.header("ìµœì¢… ìš”ì•½ë³¸")
        st.text_area("ìµœì¢… ìš”ì•½ë³¸:", final_summary, height=200)

        # í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥í•  ë‚´ìš©
        text_to_save = f"ëª¨ë“  ë…¹ìŒ ë‚´ìš©:\n{st.session_state.all_transcripts}\n\nìµœì¢… ìš”ì•½ë³¸:\n{final_summary}"
        
        # í…ìŠ¤íŠ¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
        st.download_button(
            label="ğŸ“„ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥",
            data=text_to_save,
            file_name="meeting_notes.txt",
            mime="text/plain"
        )

    if st.button("ì´ˆê¸°í™” í•˜ê¸°"):
        st.session_state.recorded = False
        st.session_state.all_transcripts = ""
        st.session_state.summaries = []
        st.success("ëª¨ë“  ë‚´ìš©ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

    st.markdown("## ìš”ì•½ë³¸ ì €ì¥ ë° ë¶ˆëŸ¬ì˜¤ê¸°")
    save_filename = st.text_input("ì €ì¥í•  íŒŒì¼ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”:", "summaries.txt")

    if st.button("ìš”ì•½ë³¸ ì €ì¥"):
        with open(save_filename, "w") as f:
            for summary in st.session_state.summaries:
                f.write(summary + "\n---\n")
        st.success(f"ìš”ì•½ë³¸ì´ {save_filename}ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    uploaded_file = st.file_uploader("ìš”ì•½ë³¸ íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°")
    if uploaded_file is not None:
        content = uploaded_file.read().decode("utf-8")
        loaded_summaries = content.split("\n---\n")
        st.session_state.summaries = loaded_summaries
        st.success("ìš”ì•½ë³¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™€ì¡ŒìŠµë‹ˆë‹¤.")

        # ë¶ˆëŸ¬ì˜¨ ìš”ì•½ë³¸ì„ í™”ë©´ì— í‘œì‹œ
        st.header("ë¶ˆëŸ¬ì˜¨ íšŒì˜ë¡ ìš”ì•½")
        for idx, summary in enumerate(st.session_state.summaries):
            st.text_area(f"ë¶ˆëŸ¬ì˜¨ íšŒì˜ë¡ ìš”ì•½ {idx + 1}:", summary, height=200)

if __name__ == "__main__":
    main()
